
Data
====
- training: pročištěný europarl, 55k vět
- dev: news11, 3k vět
- test: news13, 3k vět

- testovací data nejsou podmnožinou trénovacích

Preprocessing
=============
- europarl pročištěn od vět označených jako cs, které ale byly v jiných
  jazycích
	- s použitím langdetect a pravidel, která byla odpozorována z korpusu
- preprocessing 
	- moses skripty:
		remove-non-printing-char.perl | 
		replace-unicode-punctuation.perl |
		normalize-punctuation.perl
	- tokenizace
	- morfemizace
		- automatické dělení na morfémy, např.
			Gesetzbuch -> Gesetz@@ buch
			hlavní město Latinské -> hlav@@ ní měst@@ o Latin@@ ské
		- použita pythonovská knihovna polyglot s předtrénovanými jazykovými modely pro morfessor
		- TODO: vyzkoušet chytřejší language-specific morfemizaci
		Gesetzbuch -> gesetz ++ buch (pro němčinu)
		hlavní město -> hlav@@ @@ní měst@@ @@o (pro češtinu, "ní" jako sufix
		je něco jiného než samostatné slovo "ní")
	- TODO: změnit slovosled německých sloves:
		Ich will in dem besten Staat, der in Europa **liegt**, **wohnen**. 
			-> 
		Ich will **wohnen** in dem besten Staat, der **liegt** in Europa.

- dosud žádný truecasing (byl by potřeba velký jazykový korpus na natrénování)
ani lowercasing (přišli bychom o informace o podstatných jménech)

Postprocessing po překladu
==========================
- pouze sed 's/@@ //g' (pospojování morfémů do slov)


Analýza OOV-rate
================
- europarl jako trénovací korpus vs dev

data:
#1 preprocessed, tokenized
#2 preprocessed, tokenized, morfesized
#3 + truecasing/lowercasing (TODO)

#4 

data           |  OOV_1-grams_tokens   |  Training_set_unique_1-grams
=========================================================================
#1-cs          |  7.2 %                |  183227        (100  %)
#2-cs          |  0.3 %                |  16477         (8.9  %)
bpe100-cs      |  0.0 %                |  358
bpe1000-cs     |  0.0 %                |  1256
pbe10000-cs    |  0.0 %                |  10174         
pbe15000-cs    |  0.0 %                |  15092
pbe18000-cs    |  0.0 %                |  18021
pbe20000-cs    |  0.0 %                |  19972
pbe50000-cs    |  0.3 %                |  48307
pbe100000-cs   |  0.4 %                |  55031
pbe1000000-cs  |  0.8 %                |  64913
--------------------------------------------------------
#1-de          |  6.9 %                |  191741        (100  %)
#2-de          |  0.4 %                |  18008         (9.4  %)
bpe100-de      |  0.0 %                |  493
bpe1000-de     |  0.0 %                |  1392
pbe10000-de    |  0.0 %                |  10358
pbe15000-de    |  0.0 %                |  15303
pbe18000-de    |  0.0 %                |  18261
pbe20000-de    |  0.1 %                |  20232
pbe50000-de    |  0.1 %                |  49252
pbe100000-de   |  0.1 %                |  35029
pbe1000000-de  |  0.4 %                |  112004



Zjištění: 
- OOV rate u #2 je přijatelně nízká i takto bez truecasingu/lowercasingu


Trénování modelu
================
- použito OpenNMT-py na jedné GPU na clusteru v Saarbrückenu
- pro začátek defaultní hyperparametry

Evaluace
========
- používám skript multi-bleu z OpenNMT
- #B: baseline je frázový překlad Dana Zemana
	- kvůli porovnání jsem stáhnul jeho referenční překlad z WMT14, preprocesoval,
	  tokenizoval a evaluoval stejným způsobem jako data v mém systému
- #1: model-01/model_acc_50.69_ppl_20.13_e13.pt.cs.pred
	- model z 13. epochy prvního překladu 
	- doba trénování: asi 8 hodin


model | score
=================================================================================================
#B    | BLEU = 13.99, 47.4/19.8/9.7/4.9 (BP=0.965, ratio=0.966, hyp_len=55093, ref_len=57048)
#1    | BLEU = 4.63, 23.6/6.6/2.7/1.1 (BP=1.000, ratio=1.263, hyp_len=72079, ref_len=57048)





